apiVersion: ome.io/v1beta1
kind: FineTunedWeight
metadata:
  name: llama-3-1-lora-adapter
  namespace: llama-3-1-70b
spec:
  baseModelRef:
    name: llama-3-1-70b
  modelType: LoraAdapter
  hyperParameters:
    {
      "learning_rate": 0.0001
    }
  displayName: llama-3-1-lora-adapter
  version: "0.0.1"
  disabled: false
  vendor: "genai"
  compartmentID: "ocid1.compartment.oc1..compartment"
  configuration:
    {
      "alpha_pattern": { },
      "auto_mapping": null,
      "base_model_name_or_path": "/models/Llama-3.1-70B-Instruct",
      "bias": "none",
      "fan_in_fan_out": false,
      "inference_mode": true,
      "init_lora_weights": true,
      "layer_replication": null,
      "layers_pattern": null,
      "layers_to_transform": null,
      "loftq_config": { },
      "lora_alpha": 8,
      "lora_dropout": 0.01,
      "megatron_config": null,
      "megatron_core": "megatron.core",
      "modules_to_save": null,
      "peft_type": "LORA",
      "r": 8,
      "rank_pattern": { },
      "revision": null,
      "target_modules": [
        "v_proj",
        "up_proj",
        "q_proj",
        "gate_proj",
        "k_proj",
        "down_proj",
        "o_proj",
        "lm_head"
      ],
      "task_type": "CAUSAL_LM",
      "use_dora": false,
      "use_rslora": false
    }
  storage:
    storageUri: oci://n/idlsnvn0f2is/b/fine-tuned-sample/o/genai/llama-3-1-70b-lora-adapter
    path: /raid/fine-tuned/llama-3-1-70b-lora-adapter
  trainingJobRef:
    name: llama-3-1-70b-peft
    namespace: llama-3-1-70b
