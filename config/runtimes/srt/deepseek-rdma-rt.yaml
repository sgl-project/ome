apiVersion: ome.io/v1beta1
kind: ClusterServingRuntime
metadata:
  name: srt-deepseek-rdma
spec:
  disabled: false
  modelSizeRange:
    min: 650B
    max: 700B
  supportedModelFormats:
    - modelFormat:
        name: safetensors
        version: "1.0.0"
      version: "1.0.0"
      modelFramework:
        name: transformers
        version: "4.46.3"
      modelArchitecture: DeepseekV3ForCausalLM
      quantization: "fp8"
      autoSelect: true
      priority: 1
    - modelFormat:
        name: safetensors
        version: "1.0.0"
      version: "1.0.0"
      modelFramework:
        name: transformers
        version: "4.33.1"
      modelArchitecture: DeepseekV3ForCausalLM
      quantization: "fp8"
      autoSelect: true
      priority: 1
  protocolVersions:
    - openAI
  routerConfig:
    runner:
      name: router
      image: ghcr.io/moirai-internal/sgl-router:dev2
      resources:
        limits:
          cpu: "1"
          memory: "2Gi"
      ports:
        - containerPort: 8080
          name: http
      command:
        - sh
        - -c
        - >
          python3 -m sglang_router.launch_router
          --host "0.0.0.0"
          --port "8080"
          --service-discovery
          --service-discovery-namespace "${NAMESPACE}"
          --service-discovery-port 8080
          --selector component=engine leaderworkerset.sigs.k8s.io/worker-index=0 ome.io/inferenceservice=${INFERENCESERVICE_NAME}
      env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: INFERENCESERVICE_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['ome.io/inferenceservice']
  engineConfig:
    annotations:
      rdma.ome.io/auto-inject: "true"
      rdma.ome.io/profile: "oci-roce"
      rdma.ome.io/container-name: "ome-container"
    leader:
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      dnsPolicy: ClusterFirstWithHostNet
      hostNetwork: true
      nodeSelector:
        oci.oraclecloud.com/rdma.authenticated: "16"
        oci.oraclecloud.com/rdma.mlx_issues: "0"
        oke.oraclecloud.com/pool.mode: cluster-network
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node.kubernetes.io/instance-type
                    operator: In
                    values:
                      - BM.GPU.H100.8
      runner:
        name: ome-container
        image: ghcr.io/moirai-internal/sgl:dev2
        command:
          - sh
          - -c
          - >
            python3 -m sglang.launch_server 
            --host 0.0.0.0 --port 8080 
            --model-path ${MODEL_PATH} 
            --tp 16 
            --nccl-init $(LWS_LEADER_ADDRESS):5000 
            --nnodes ${LWS_GROUP_SIZE} 
            --node-rank ${LWS_WORKER_INDEX} 
            --trust-remote-code 
            --enable-torch-compile 
            --torch-compile-max-bs 1 
            --reasoning-parser deepseek-r1 
            --enable-metrics
        resources:
          requests:
            nvidia.com/gpu: 8
          limits:
            nvidia.com/gpu: 8
        readinessProbe:
          httpGet:
            path: /health_generate
            port: 8080
          failureThreshold: 3
          successThreshold: 1
          periodSeconds: 60
          timeoutSeconds: 200
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          failureThreshold: 5
          successThreshold: 1
          periodSeconds: 60
          timeoutSeconds: 60
        startupProbe:
          httpGet:
            path: /health_generate
            port: 8080
          failureThreshold: 300
          successThreshold: 1
          periodSeconds: 10
          initialDelaySeconds: 300
          timeoutSeconds: 30
    worker:
      size: 1
      nodeSelector:
        oci.oraclecloud.com/rdma.authenticated: "16"
        oci.oraclecloud.com/rdma.mlx_issues: "0"
        oke.oraclecloud.com/pool.mode: cluster-network
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      dnsPolicy: ClusterFirstWithHostNet
      hostNetwork: true
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node.kubernetes.io/instance-type
                    operator: In
                    values:
                      - BM.GPU.H100.8
      runner:
        name: ome-container
        image: ghcr.io/moirai-internal/sgl:dev2
        command:
          - sh
          - -c
          - >
            python3 -m sglang.launch_server 
            --host 0.0.0.0 
            --port 8080 
            --model-path ${MODEL_PATH} 
            --tp 16 
            --nccl-init $(LWS_LEADER_ADDRESS):5000 
            --nnodes ${LWS_GROUP_SIZE} 
            --node-rank ${LWS_WORKER_INDEX} 
            --trust-remote-code 
            --enable-torch-compile 
            --torch-compile-max-bs 1 
            --reasoning-parser deepseek-r1 
            --enable-metrics
        resources:
          limits:
            nvidia.com/gpu: "8"
          requests:
            nvidia.com/gpu: "8"