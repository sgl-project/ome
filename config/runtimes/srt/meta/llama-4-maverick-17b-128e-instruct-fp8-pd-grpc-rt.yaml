apiVersion: ome.io/v1beta1
kind: ClusterServingRuntime
metadata:
  name: srt-llama-4-maverick-17b-128e-instruct-fp8-pd-grpc
spec:
  disabled: false
  supportedModelFormats:
    - modelFramework:
        name: transformers
        version: "4.51.0"
      modelFormat:
        name: safetensors
        version: "1.0.0"
      modelArchitecture: Llama4ForConditionalGeneration
      autoSelect: false
      priority: 2
  protocolVersions:
    - openAI
  modelSizeRange:
    min: 400B
    max: 402B
  engineConfig:
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "8080"
      prometheus.io/path: "/metrics"
    labels:
      logging-forward: enabled
    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
    volumes:
      - name: dshm
        emptyDir:
          medium: Memory
    dnsPolicy: ClusterFirstWithHostNet
    hostNetwork: true
    runner:
      name: ome-container
      image: docker.io/lmsysorg/sglang:v0.5.5.post3-cu129-amd64
      ports:
        - containerPort: 8080
          name: grpc1
          protocol: TCP
      command:
        - python3
        - -m
        - sglang.launch_server
        - --host
        - "0.0.0.0"
        - --port
        - "8080"
        - --enable-metrics
        - --model-path
        - $(MODEL_PATH)
        - --disaggregation-mode
        - prefill
        - --disaggregation-ib-device
        - mlx5_0
        - --tp-size
        - "8"
        - --context-length
        - "430000"
        - --chat-template
        - llama-4
        - --attention-backend
        - fa3
        - --log-requests
        - --grpc-mode
        - --served-model-name
        - meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
      resources:
        requests:
          cpu: 128
          memory: 512Gi
          nvidia.com/gpu: 8
        limits:
          cpu: 128
          memory: 512Gi
          nvidia.com/gpu: 8
      # Liveness: Restart if process dies
      livenessProbe:
        grpc:
          port: 8080
          service: ""  # Overall server health
        failureThreshold: 5
        successThreshold: 1
        periodSeconds: 60
        timeoutSeconds: 60

      # Readiness: Don't send traffic until ready
      readinessProbe:
        grpc:
          port: 8080
          service: "sglang.grpc.scheduler.SglangScheduler"
        initialDelaySeconds: 60
        failureThreshold: 3
        successThreshold: 1
        periodSeconds: 60
        timeoutSeconds: 200

      startupProbe:
        grpc:
          port: 8080
          service: ""
        failureThreshold: 150
        successThreshold: 1
        periodSeconds: 6
        initialDelaySeconds: 60
        timeoutSeconds: 30
  decoderConfig:
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "8080"
      prometheus.io/path: "/metrics"
    labels:
      logging-forward: enabled
    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
    volumes:
      - name: dshm
        emptyDir:
          medium: Memory
    dnsPolicy: ClusterFirstWithHostNet
    hostNetwork: true
    runner:
      name: ome-container
      image: docker.io/lmsysorg/sglang:v0.5.5.post3-cu129-amd64
      ports:
        - containerPort: 8080
          name: grpc1
          protocol: TCP
      command:
        - python3
        - -m
        - sglang.launch_server
        - --host
        - "0.0.0.0"
        - --port
        - "8080"
        - --enable-metrics
        - --model-path
        - $(MODEL_PATH)
        - --disaggregation-mode
        - decode
        - --disaggregation-ib-device
        - mlx5_0
        - --tp-size
        - "8"
        - --context-length
        - "430000"
        - --chat-template
        - llama-4
        - --attention-backend
        - fa3
        - --log-requests
        - --grpc-mode
        - --served-model-name
        - meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
      resources:
        requests:
          cpu: 128
          memory: 512Gi
          nvidia.com/gpu: 8
        limits:
          cpu: 128
          memory: 512Gi
          nvidia.com/gpu: 8
      # Liveness: Restart if process dies
      livenessProbe:
        grpc:
          port: 8080
          service: ""  # Overall server health
        failureThreshold: 5
        successThreshold: 1
        periodSeconds: 60
        timeoutSeconds: 60

      # Readiness: Don't send traffic until ready
      readinessProbe:
        grpc:
          port: 8080
          service: "sglang.grpc.scheduler.SglangScheduler"
        initialDelaySeconds: 60
        failureThreshold: 3
        successThreshold: 1
        periodSeconds: 60
        timeoutSeconds: 200

      startupProbe:
        grpc:
          port: 8080
          service: ""
        failureThreshold: 150
        successThreshold: 1
        periodSeconds: 6
        initialDelaySeconds: 60
        timeoutSeconds: 30
  routerConfig:
    runner:
      name: router
      image: docker.io/lmsysorg/sglang-router:v0.2.3
      resources:
        limits:
          cpu: "1"
          memory: "2Gi"
      ports:
        - containerPort: 8080
          name: http
      command:
        - python3
        - -m
        - sglang_router.launch_router
        - --host
        - "0.0.0.0"
        - --port
        - "8080"
        - --model-path
        - /raid/models/meta/llama-4-maverick-17b-128e-instruct-fp8
        - --pd-disaggregation
        - --policy
        - power_of_two
        - --service-discovery
        - --service-discovery-namespace
        - $(NAMESPACE)
        - --service-discovery-port
        - "8080"
        - --prefill-selector
        - component=engine ome.io/inferenceservice=$(INFERENCESERVICE_NAME)
        - --decode-selector
        - component=decoder ome.io/inferenceservice=$(INFERENCESERVICE_NAME)
      env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: INFERENCESERVICE_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['ome.io/inferenceservice']
