apiVersion: ome.io/v1beta1
kind: ClusterServingRuntime
metadata:
  name: srt-llama-4-maverick-17b-128e-instruct-fp8-pd
spec:
  disabled: false
  supportedModelFormats:
    - modelFramework:
        name: transformers
        version: "4.51.0.dev0"
      modelFormat:
        name: safetensors
        version: "1.0.0"
      modelArchitecture: Llama4ForConditionalGeneration
      autoSelect: false
      priority: 2
  protocolVersions:
    - openAI
  modelSizeRange:
    min: 400B
    max: 402B
  engineConfig:
    annotations:
      rdma.ome.io/auto-inject: "true"
      rdma.ome.io/profile: "oci-roce"
      rdma.ome.io/container-name: "ome-container"
      prometheus.io/scrape: "true"
      prometheus.io/port: "8080"
      prometheus.io/path: "/metrics"
    labels:
      logging-forward: enabled
    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
    volumes:
      - name: dshm
        emptyDir:
          medium: Memory
    dnsPolicy: ClusterFirstWithHostNet
    hostNetwork: true
    runner:
      name: ome-container
      image: docker.io/lmsysorg/sglang:v0.4.8.post1-cu126
      ports:
        - containerPort: 8080
          name: http1
          protocol: TCP
      command:
        - /bin/bash
        - '-lc'
        - --
      args:
        - |
          python3 -m sglang.launch_server \
          --host=0.0.0.0 \
          --port=8080 \
          --enable-metrics \
          --model-path="$MODEL_PATH" \
          --disaggregation-mode prefill \
          --disaggregation-ib-device mlx5_0 \
          --tp-size 8 \
          --context-length=430000 \
          --chat-template llama-4 \
          --attention-backend fa3 \
          --log-requests
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
      resources:
        requests:
          cpu: 128
          memory: 512Gi
          nvidia.com/gpu: 8
        limits:
          cpu: 128
          memory: 512Gi
          nvidia.com/gpu: 8
      readinessProbe:
        httpGet:
          path: /health_generate
          port: 8080
        failureThreshold: 3
        successThreshold: 1
        periodSeconds: 60
        timeoutSeconds: 200
      livenessProbe:
        httpGet:
          path: /health
          port: 8080
        failureThreshold: 5
        successThreshold: 1
        periodSeconds: 60
        timeoutSeconds: 60
      startupProbe:
        httpGet:
          path: /health_generate
          port: 8080
        failureThreshold: 150
        successThreshold: 1
        periodSeconds: 6
        initialDelaySeconds: 60
        timeoutSeconds: 30
  decoderConfig:
    annotations:
      rdma.ome.io/auto-inject: "true"
      rdma.ome.io/profile: "oci-roce"
      rdma.ome.io/container-name: "ome-container"
      prometheus.io/scrape: "true"
      prometheus.io/port: "8080"
      prometheus.io/path: "/metrics"
    labels:
      logging-forward: enabled
    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
    volumes:
      - name: dshm
        emptyDir:
          medium: Memory
    dnsPolicy: ClusterFirstWithHostNet
    hostNetwork: true
    runner:
      name: ome-container
      image: docker.io/lmsysorg/sglang:v0.4.8.post1-cu126
      ports:
        - containerPort: 8080
          name: http1
          protocol: TCP
      command:
        - /bin/bash
        - '-lc'
        - --
      args:
        - |
          python3 -m sglang.launch_server \
          --host=0.0.0.0 \
          --port=8080 \
          --enable-metrics \
          --model-path="$MODEL_PATH" \
          --disaggregation-mode decode \
          --disaggregation-ib-device mlx5_0 \
          --tp-size 8 \
          --context-length=430000 \
          --chat-template llama-4 \
          --attention-backend fa3 \
          --log-requests
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
      resources:
        requests:
          cpu: 128
          memory: 512Gi
          nvidia.com/gpu: 8
        limits:
          cpu: 128
          memory: 512Gi
          nvidia.com/gpu: 8
      readinessProbe:
        httpGet:
          path: /health_generate
          port: 8080
        failureThreshold: 3
        successThreshold: 1
        periodSeconds: 60
        timeoutSeconds: 200
      livenessProbe:
        httpGet:
          path: /health
          port: 8080
        failureThreshold: 5
        successThreshold: 1
        periodSeconds: 60
        timeoutSeconds: 60
      startupProbe:
        httpGet:
          path: /health_generate
          port: 8080
        failureThreshold: 150
        successThreshold: 1
        periodSeconds: 6
        initialDelaySeconds: 60
        timeoutSeconds: 30
  routerConfig:
    runner:
      name: router
      image: ghcr.io/moirai-internal/sgl-router:0.1.4.30f2a44
      resources:
        limits:
          cpu: "1"
          memory: "2Gi"
      ports:
        - containerPort: 8080
          name: http
      command:
        - sh
        - -c
        - >
          python3 -m sglang_router.launch_router
          --host "0.0.0.0"
          --port "8080"
          --pd-disaggregation
          --policy power_of_two
          --service-discovery
          --service-discovery-namespace "${NAMESPACE}"
          --service-discovery-port 8080
          --prefill-selector component=engine ome.io/inferenceservice=${INFERENCESERVICE_NAME}
          --decode-selector component=decoder ome.io/inferenceservice=${INFERENCESERVICE_NAME}
      env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: INFERENCESERVICE_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['ome.io/inferenceservice']
