apiVersion: ome.io/v1beta1
kind: ClusterServingRuntime
metadata:
  name: cohere-embed-multi-modal
spec:
  disabled: false
  annotations:
    prometheus.io/port: '8081'
    prometheus.io/scrape: 'true'
    ome.io/disable-blocklist: 'true'
    ome.io/inject-model-init: 'true'
  labels:
    logging-forward: enabled
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  supportedModelFormats:
    - autoSelect: true
      modelArchitecture: CohereForCausalLM
      version: "3.0"
      modelFormat:
        name: onnx
        version: '1'
      modelFramework:
        name: onnxruntime
        version: '1'
    - autoSelect: true
      modelArchitecture: CohereForCausalLM
      version: "4.0"
      modelFormat:
        name: onnx
        version: '1'
      modelFramework:
        name: onnxruntime
        version: '1'
  modelSizeRange:
    min: 610M
    max: 690M
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                  - BM.GPU.A10.4
                  - BM.GPU.B4.8
                  - BM.GPU4.8
                  - BM.GPU.A100-v2.8
                  - BM.GPU.H100.8
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                  - BM.GPU.A10.4
        - weight: 50
          preference:
            matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                  - BM.GPU.B4.8
                  - BM.GPU4.8
                  - BM.GPU.A100-v2.8
        - weight: 1
          preference:
            matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                  - BM.GPU.H100.8
  protocolVersions:
    - cohere
#  schedulerName: {{ .Values.commonSchedulerName }}
  volumes:
    - name: shared-memory
      emptyDir:
        medium: Memory
        sizeLimit: 1Gi
    - name: model-empty-dir
      emptyDir:
        medium: Memory
  containers:
    - name: ome-container
      image: us-chicago-1.ocir.io/idqj093njucb/cohere-oci-all:v0.8.1-04012025
      ports:
        - containerPort: 8080
          name: http1
      env:
        - name: ENDPOINT
          value: "embed"
      resources:
        limits:
          nvidia.com/gpu: "1"
        requests:
          cpu: 4
          memory: 120Gi
          nvidia.com/gpu: "1"
      volumeMounts:
        - mountPath: /dev/shm
          name: shared-memory
        - mountPath: /opt/ml/model
          name: model-empty-dir
      readinessProbe:
        httpGet:
          path: /ping
          port: 8080
        failureThreshold: 5
        successThreshold: 1
        periodSeconds: 10
        timeoutSeconds: 20
      livenessProbe:
        httpGet:
          path: /ping
          port: 8080
        failureThreshold: 5
        successThreshold: 1
        periodSeconds: 10
        timeoutSeconds: 20
      startupProbe:
        httpGet:
          path: /ping
          port: 8080
        failureThreshold: 100
        successThreshold: 1
        periodSeconds: 3
        initialDelaySeconds: 60
        timeoutSeconds: 5
