# This runtime is for llama-3-3-70b-instruct, although all the parameters are the same as llama-3-1-70b-instruct
# adding this for the sake of completeness and clarity \_(ツ)_/¯ \_(ツ)_/¯ \_(ツ)_/¯ \_(ツ)_/¯ \_(ツ)_/¯
# five \_(ツ)_/¯ is too much? dont make me add more
# so we don't hear complains about "WHERE IS THE Llama-3-3-70b-instruct"
apiVersion: ome.io/v1beta1
kind: ClusterServingRuntime
metadata:
  name: vllm-llama-3-3-70b-instruct
spec:
  disabled: false
  supportedModelFormats:
    - modelFramework:
        name: transformers
        version: "4.47.0.dev0"
      modelFormat:
        name: safetensors
        version: "1.0.0"
      modelArchitecture: LlamaForCausalLM
      autoSelect: true
      priority: 1
      version: "1.0.0"
  protocolVersions:
    - openAI
  modelSizeRange:
    min: 60B
    max: 75B
  engineConfig:
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "8080"
      prometheus.io/path: "/metrics"
    labels:
      logging-forward: enabled
    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
    volumes:
      - name: dshm
        emptyDir:
          medium: Memory
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: node.kubernetes.io/instance-type
                  operator: In
                  values:
                    - BM.GPU.B4.8
                    - BM.GPU4.8
                    - BM.GPU.A100-v2.8
                    - BM.GPU.H100.8
    runner:
      name: ome-container
      image: docker.io/vllm/vllm-openai:v0.9.0.1
      ports:
        - containerPort: 8080
          name: http1
          protocol: TCP
      command:
        - /bin/bash
        - '-lc'
        - --
      args:
        - |
          python3 -m vllm.entrypoints.openai.api_server \
          --port=8080 \
          --model="$MODEL_PATH" \
          --middleware=vllm.entrypoints.openai.middleware.log_opc_header \
          --max-log-len=0 \
          --served-model-name="$SERVED_MODEL_NAME" \
          --tensor-parallel-size=4 \
          --max-model-len=131072 \
          --gpu-memory-utilization=0.9 \
          --enable-chunked-prefill \
          --enable-auto-tool-choice \
          --tool-call-parser=llama3_json \
          --chat-template=./examples/tool_chat_template_llama3.1_json.jinja
      env:
        - name: SERVED_MODEL_NAME
          value: "vllm-model"
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
      resources:
        requests:
          cpu: 30
          memory: 100Gi
          nvidia.com/gpu: 4
        limits:
          cpu: 30
          memory: 100Gi
          nvidia.com/gpu: 4

      readinessProbe:
        httpGet:
          path: /health
          port: 8080
        failureThreshold: 3
        successThreshold: 1
        periodSeconds: 60
        timeoutSeconds: 200

      livenessProbe:
        httpGet:
          path: /health
          port: 8080
        failureThreshold: 5
        successThreshold: 1
        periodSeconds: 60
        timeoutSeconds: 60

      startupProbe:
        httpGet:
          path: /health
          port: 8080
        failureThreshold: 150
        successThreshold: 1
        periodSeconds: 6
        initialDelaySeconds: 60
        timeoutSeconds: 30