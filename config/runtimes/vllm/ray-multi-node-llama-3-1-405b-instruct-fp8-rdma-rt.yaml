apiVersion: ome.io/v1beta1
kind: ClusterServingRuntime
metadata:
  name: vllm-ray-multi-node-llama-3-1-405b-instruct-fp8-rdma
spec:
  disabled: false
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
    prometheus.io/path: "/metrics"
    linkerd.io/inject: "enabled"
    config.linkerd.io/skip-inbound-ports: "1-8079,8081-65535"
    config.linkerd.io/skip-outbound-ports: "1-8079,8081-65535"
    ome.io/deploymentMode: MultiNodeRayVLLM
  labels:
    logging-forward: enabled
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  dnsPolicy: ClusterFirstWithHostNet
  hostNetwork: true
  nodeSelector:
    oci.oraclecloud.com/rdma.authenticated: "16"
    oci.oraclecloud.com/rdma.mlx_issues: "0"
    oke.oraclecloud.com/pool.mode: cluster-network
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                  - BM.GPU.H100.8
  pipelineParallelism: true
  modelSizeRange:
    min: 400B
    max: 420B
  supportedModelFormats:
    - modelFramework:
        name: transformers
        version: "4.43.0.dev0"
      modelFormat:
        name: safetensors
        version: "1"
      modelArchitecture: LlamaForCausalLM
      autoSelect: false
      priority: 1
      version: "1.0.0"

  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
    - hostPath:
        path: /dev/infiniband
      name: devinf
    - emptyDir: { }
      name: log-volume
  protocolVersions:
    - openAI
  containers:
    - name: ome-container
      image: fra.ocir.io/idqj093njucb/official-vllm:v0.6.4.post1.0c9082a1
      env:
        - name: NCCL_NET_PLUGIN
          value: none
        - name: NCCL_DEBUG
          value: INFO
        - name: NCCL_CROSS_NIC
          value: '2'
        - name: NCCL_SOCKET_NTHREADS
          value: '16'
        - name: NCCL_CUMEM_ENABLE
          value: '0'
        - name: NCCL_IB_SPLIT_DATA_ON_QPS
          value: '0'
        - name: NCCL_IB_QPS_PER_CONNECTION
          value: '16'
        - name: NCCL_IB_GID_INDEX
          value: '3'
        - name: NCCL_IB_HCA
          value: "=mlx5_0,mlx5_1,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7,mlx5_8,mlx5_9,mlx5_10,mlx5_12,mlx5_13,mlx5_14,mlx5_15,mlx5_16,mlx5_17"
        - name: NCCL_IB_TC
          value: '41'
        - name: NCCL_IB_SL
          value: '0'
        - name: NCCL_IB_TIMEOUT
          value: '22'
        - name: HCOLL_ENABLE_MCAST_ALL
          value: '0'
        - name: coll_hcoll_enable
          value: '0'
        - name: UCX_TLS
          value: tcp
        - name: UCX_NET_DEVICES
          value: eth0
        - name: RX_QUEUE_LEN
          value: '8192'
        - name: IB_RX_QUEUE_LEN
          value: '8192'
        - name: NCCL_SOCKET_IFNAME
          value: eth0
        - name: NCCL_IGNORE_CPU_AFFINITY
          value: '1'
        - name: GLOO_SOCKET_IFNAME
          value: eth0
        - name: VLLM_PORT
          value: "9000"
      command:
        - /bin/bash
        - '-lc'
        - --
      args:
        - |
          ulimit -n 65536;
          eval "$KUBERAY_GEN_RAY_START_CMD" &
          (
            while ! ray status | grep -q '0.0/16.0 GPU'; do
              echo 'Waiting for GPUs to be available...';
              sleep 2;
            done;
            python3 -m vllm.entrypoints.openai.api_server \
              --port=8080 \
              --model="$MODEL_PATH" \
              --max-log-len=0 \
              --middleware=vllm.entrypoints.openai.middleware.log_opc_header \
              --served-model-name=vllm-model \
              --tensor-parallel-size=8 \
              --pipeline-parallel-size=2 \
              --enable-chunked-prefill
          ) &
          wait
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - mountPath: /dev/infiniband
          name: devinf
        - mountPath: /tmp/ray
          name: log-volume
      resources:
        requests:
          cpu: 128
          memory: 216Gi
          nvidia.com/gpu: 8
        limits:
          cpu: 128
          memory: 216Gi
          nvidia.com/gpu: 8
      securityContext:
        capabilities:
          add:
            - IPC_LOCK
            - CAP_SYS_ADMIN
        privileged: true
      livenessProbe:
        exec:
          command:
            - sh
            - '-c'
            - echo success
        initialDelaySeconds: 10
        periodSeconds: 10
      readinessProbe:
        exec:
          command:
            - sh
            - '-c'
            - echo success
        initialDelaySeconds: 10
        periodSeconds: 10