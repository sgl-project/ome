apiVersion: ome.io/v1beta1
kind: ClusterBaseModel
metadata:
  name: llama-4-scout-17b-16e-instruct
spec:
  vendor: meta
  disabled: false
  displayName: meta.llama-4-scout-17b-16e-instruct
  version: "1.0.0"
  modelFramework:
    name: transformers
    version: "4.51.0.dev0"
  modelFormat:
    name: safetensors
    version: "1"
  modelArchitecture: Llama4ForConditionalGeneration
  modelParameterSize: 109B
  maxTokens: 10485760
  modelCapabilities:
    - "CHAT"
  storage:
    storageUri: oci://n/idqj093njucb/b/model-store/o/meta/llama-4-scout-17b-16e-instruct
    path: /raid/models/meta/llama-4-scout-17b-16e-instruct
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                  - BM.GPU.H100.8
                  - BM.GPU4.8
                  - BM.GPU.A100-v2.8
  modelConfiguration:
    {
      "architectures": [
        "Llama4ForConditionalGeneration"
      ],
      "boi_token_index": 200080,
      "eoi_token_index": 200081,
      "image_token_index": 200092,
      "model_type": "llama4",
      "text_config": {
        "_attn_implementation_autoset": true,
        "attention_bias": false,
        "attention_chunk_size": 8192,
        "attention_dropout": 0.0,
        "bos_token_id": 200000,
        "eos_token_id": [
          200001,
          200007,
          200008
        ],
        "for_llm_compressor": false,
        "head_dim": 128,
        "hidden_act": "silu",
        "hidden_size": 5120,
        "initializer_range": 0.02,
        "interleave_moe_layer_step": 1,
        "intermediate_size": 8192,
        "intermediate_size_mlp": 16384,
        "max_position_embeddings": 10485760,
        "model_type": "llama4_text",
        "no_rope_layers": [ ],
        "num_attention_heads": 40,
        "num_experts_per_tok": 1,
        "num_hidden_layers": 48,
        "num_key_value_heads": 8,
        "num_local_experts": 16,
        "output_router_logits": false,
        "pad_token_id": 200018,
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
          "factor": 8.0,
          "high_freq_factor": 4.0,
          "low_freq_factor": 1.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "rope_theta": 500000.0,
        "router_aux_loss_coef": 0.001,
        "router_jitter_noise": 0.0,
        "torch_dtype": "bfloat16",
        "use_cache": true,
        "use_qk_norm": true,
        "vocab_size": 202048
      },
      "torch_dtype": "bfloat16",
      "transformers_version": "4.51.0.dev0",
      "vision_config": {
        "_attn_implementation_autoset": true,
        "attention_dropout": 0.0,
        "hidden_act": "gelu",
        "hidden_size": 1408,
        "image_size": 336,
        "initializer_range": 0.02,
        "intermediate_size": 5632,
        "model_type": "llama4_vision_model",
        "multi_modal_projector_bias": false,
        "norm_eps": 1e-05,
        "num_attention_heads": 16,
        "num_channels": 3,
        "num_hidden_layers": 34,
        "patch_size": 14,
        "pixel_shuffle_ratio": 0.5,
        "projector_dropout": 0.0,
        "projector_input_dim": 4096,
        "projector_output_dim": 4096,
        "rope_theta": 10000,
        "vision_feature_layer": -1,
        "vision_feature_select_strategy": "default",
        "vision_output_dim": 4096
      }
    }