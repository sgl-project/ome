apiVersion: ome.io/v1beta1
kind: ClusterServingRuntime
metadata:
  name: vllm-ray-multi-node-deepseek-v3
spec:
  disabled: true
  {{- if or .Values.commonAnnotations .Values.deepseek_v3.annotations }}
  annotations:
    {{- if .Values.commonAnnotations }}
    {{ toYaml .Values.commonAnnotations | nindent 6 }}
    {{- end }}
    {{- if .Values.deepseek_v3.annotations }}
    {{ toYaml .Values.deepseek_v3.annotations | nindent 6 }}
    {{- end }}
  {{- end }}
  labels:
    logging-forward: enabled
    {{- if .Values.deepseek_v3.labels }}
    {{ toYaml .Values.deepseek_v3.labels | nindent 6 }}
    {{- end }}
  {{- if or .Values.commonTolerations .Values.deepseek_v3.tolerations }}
  tolerations:
    {{- if .Values.commonTolerations }}
    {{ toYaml .Values.commonTolerations | nindent 6 }}
    {{- end }}
    {{- if .Values.deepseek_v3.tolerations }}
    {{ toYaml .Values.deepseek_v3.tolerations | nindent 6 }}
    {{- end }}
  {{- end }}
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                  - BM.GPU.H100.8
  supportedModelFormats:
    - modelFormat:
        name: safetensors
        version: "1"
      modelFramework:
        name: transformers
        version: "4.33.1"
      modelArchitecture: DeepseekV3ForCausalLM
      autoSelect: true
      priority: 1
  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
    - emptyDir: { }
      name: log-volume
  protocolVersions:
    - openAI
  modelSizeRange:
    min: 650B
    max: 700B
  containers:
    - name: ome-container
      image: {{ .Values.deepseek_v3.image.repository | default .Values.vllm.commonImage.repository }}:{{ .Values.deepseek_v3.image.tag | default .Values.vllm.commonImage.tag }}
      env:
        - name: VLLM_PORT
          value: "9000"
      command:
        - /bin/bash
        - '-lc'
        - --
      args:
        - |
          ulimit -n 65536;
          eval "$KUBERAY_GEN_RAY_START_CMD" &
          (
            while ! ray status | grep -q '0.0/16.0 GPU'; do
              echo 'Waiting for GPUs to be available...';
              sleep 2;
            done;
            python3 -m vllm.entrypoints.openai.api_server \
              --port={{ .Values.vllm.port }} \
              --model="$MODEL_PATH" \
              --max-log-len=0 \
              --middleware=vllm.entrypoints.openai.middleware.log_opc_header \
              --served-model-name={{ .Values.vllm.serveModelName }} \
              --tensor-parallel-size=8 \
              --pipeline-parallel-size=2 \
              --trust-remote-code \
              --gpu-memory-utilization=0.95 \
              --enable-chunked-prefill
          ) &
          wait
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - mountPath: /tmp/ray
          name: log-volume
      resources:
        requests:
          cpu: 128
          memory: 216Gi
          nvidia.com/gpu: 8
        limits:
          cpu: 128
          memory: 216Gi
          nvidia.com/gpu: 8
      livenessProbe:
        exec:
          command:
            - sh
            - '-c'
            - echo success
        initialDelaySeconds: 10
        periodSeconds: 10
      readinessProbe:
        exec:
          command:
            - sh
            - '-c'
            - echo success
        initialDelaySeconds: 10
        periodSeconds: 10
